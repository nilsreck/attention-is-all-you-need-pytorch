@misc{vaswani2024attentionneed, title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit
            and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia
            Polosukhin}, year={2023}, eprint={1706.03762},
            archivePrefix={arXiv}, primaryClass={cs.CL},
            url={https://arxiv.org/abs/1706.03762}, }
    
@misc{press2017usingoutputembeddingimprove, title={Using the Output Embedding
    to Improve Language Models}, author={Ofir Press and Lior Wolf},
    year={2017}, eprint={1608.05859}, archivePrefix={arXiv},
    primaryClass={cs.CL}, url={https://arxiv.org/abs/1608.05859}, }

@misc{he2015deepresiduallearningimage, title={Deep Residual Learning for Image
    Recognition}, author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and
    Jian Sun}, year={2015}, eprint={1512.03385}, archivePrefix={arXiv},
    primaryClass={cs.CV}, url={https://arxiv.org/abs/1512.03385}, }

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@article{Popel_2018,
   title={Training Tips for the Transformer Model},
   volume={110},
   ISSN={1804-0462},
   url={http://dx.doi.org/10.2478/pralin-2018-0002},
   DOI={10.2478/pralin-2018-0002},
   number={1},
   journal={The Prague Bulletin of Mathematical Linguistics},
   publisher={Charles University in Prague, Karolinum Press},
   author={Popel, Martin and Bojar, Ondřej},
   year={2018},
   month=apr, pages={43–70} }

@misc{ba2016layernormalization,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}
