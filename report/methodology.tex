\section{Methodology} The Transformer mainly consists of two principal components, the encoder and decoder.
While both process and transform input data, the encoder focuses on generating a context-rich representation of the input sequence, whereas the decoder uses this representation to generate the target sequence step by step.
However, before the model can process the input data, it has to be encoded in a numerical representation that the model can interpret.
For this, a shared tokenizer is trained over the source and target sequences, which maps a token (a word or subword) of a sequence to a number and vice versa. 

\todo[inline]{add info about alignment of sequences (maybe add to training section)}

\subsection{Embedding Layers} 
The embedding layer creates a \texttt{$d_{model}$}-dimensional vector representation for each encoded token of the input and target sequence. 
Unlike recurrent architectures, which process sequences step by step, the Transformer processes entire sequences in parallel. 
To compensate for the lack of sequence order awareness, the positional encoding layer enriches the representations with positional information. \todo{How does the model differentiate between embedding and position?} \\
Cosistent with the original Transfromer architecture, we apply parameter sharing by using the same set of weights for both embedding layers and the pre-softmax linear transformation. This technique has shown to improve efficiency and model performance~\cite{press2017usingoutputembeddingimprove}.
Sharing parameters between the encoder and decoder embedding layers offers several advantages.
First, it can significantly reduce the model size while maintaining model performance.
Second, parmeter sharing reduces the degrees of freedom of the model, thus implicitly applying regularization by forcing different parts of the model to use the same parameters, preventing the model from overfitting.
Additionally, the efficiency of the model improves because shared parameters allow for faster updates and fewer memory operations.
Finally, by tying the input and output embeddings together, the model can enhance cross-lingual transfer learning, as aligned word representations across languages make it easier to generalize.

\subsection{Encoder Stack} 
The encoder consists of six identical layers, each designed to transform the input sequence into a context-rich representation.
Each layer comprises two sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network, each followed by a residual connection~\cite{he2015deepresiduallearningimage} and layer normalization~\cite{ba2016layernormalization} to stabilize training and improve gradient flow.
Residual connections, defined as  \(y = \mathbf{x} + f(\mathbf{x})\), preserve the original signal while adding important features from multi-head attention or feed-forward layers, alleviating the problem of vanishing gradients during backpropagation.
If the transformation \(f(\mathbf{x})\) collapses to zero (e.g. due to all weights and biases being pushed to zero), the output reduces to \(y = \mathbf{x}\), ensuring that the original signal is preserved when the layer does not learn anything.
Residual connections can also be described by the residual mapping \(g(\mathbf{x}) = f(\mathbf{x}) - \mathbf{x}\), emphasizing that the network only needs to learn a small transformation when \(f(\mathbf{x})\) is close to the identity function.
Learning a function close to the identity function, residual blocks slightly refine existing features instead of learning full (high variance) functions from scratch. \\
Additionally, by focussing on small residuals \(g(\mathbf{x}) = f(\mathbf{x}) - \mathbf{x}\), the network increases the chance of generalizing better to unseen data, reducing overfitting.\todo{why?}

\subsection{Decoder Stack} 
The decoder also consists of six identical layers.
In addition to the two sub-layers of the encoder, it has a second multi-head attention mechanism over the outputs of the encoder.
Consistent with the encoder, residual connections and layer normalization are employed after each sub-layer.
In contrast to the multi-head self-attention layer in the encoder, the inputs to the attention mechanism in the decoder are masked such that the decoder cannot attend to future tokens.
This prevents the decoder from cheating by attending to tokens it has not yet seen.
Finally, the output of the decoder undergoes a linear transformation. After that, softmax is applied to convert the output into probabilities to predict the next token.


% why position? Understand embeddings of the input. Add how the heads are
% concatenated in attention
\subsection{Attention} 
\todo[inline]{Explain how multiple atttention heads help}
It expands the model’s ability to focus on different positions.
Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself.
If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.

\subsection{Position-wise Feed-Forward Layer}
% layer normalization where? Write down the equation for the layer
% normalization layer and provide an explanation of the function of this layer
% in a transformer model. My question: Why is it important to use two
% independent layer normalization layers following multi-head self-attention
% layer and the position-wise feed-forward layer? Explain the benefits of
% parameter sharing in the transformer model.

\begin{center}
\begin{tikzpicture}[
		>={Stealth[round]}, % Arrow style
		node distance=2.5cm, % Distance between nodes
		every node/.style={draw, rounded corners, minimum width=2.5cm, minimum height=1.2cm, font=\small}
	]
			
	% Input node
	\node[draw=none] (input) {$x_i$};
			
	% Linear layer nodes
	\node[right=1cm of input] (K) {$W_K \cdot x_i + b_K$};
	\node[above=0.8cm of K] (Q) {$W_Q \cdot x_i + b_Q$};
	\node[below=0.8cm of K] (V) {$W_V \cdot x_i + b_V$};
			
	% Connect input to each linear layer
	\draw[->] (input.east) |- (Q.west);
	\draw[->] (input.east) |- (K.west);
	\draw[->] (input.east) |- (V.west);
			
	% Label nodes
	\node[below=0.2cm of V, draw=none] (labels) {Three parallel linear transformations};
			
	% Concatenate box with vertical text
	\node[draw, right=1cm of K, minimum width=1.2cm, minimum height=4cm, align=center] (concatBox) {Concatenate};
			
	% Connect linear layers to the concatenate box
	\draw[->] (Q.east) -- ++(0.4,0) |- (concatBox.west);
	\draw[->] (K.east) -- ++(0.4,0) |- (concatBox.west);
	\draw[->] (V.east) -- ++(0.4,0) |- (concatBox.west);
			
	% Dashed QKV outputs box centered vertically with Concatenate box
	\node[draw, dashed, right=1cm of concatBox, minimum width=2.5cm, minimum height=1.2cm, font=\small] (QKVBox) {Q, K, V outputs};
			
	% Arrow to QKV outputs
	\draw[->, dashed] (concatBox.east) -- (QKVBox.west);
			
\end{tikzpicture}
\end{center}
