\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{press2017usingoutputembeddingimprove}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{1}{section.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{add info about alignment of sequences (maybe add to training section)}{1}{section*.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{20088094}{25607419}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Embedding Layers}{1}{subsection.2.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{delete last point?}{1}{section*.2}\protected@file@percent }
\pgfsyspdfmark {pgfid2}{15069137}{11202013}
\pgfsyspdfmark {pgfid5}{34243870}{11214301}
\pgfsyspdfmark {pgfid6}{36357406}{10990387}
\@writefile{tdo}{\contentsline {todo}{How does the model differentiate between embedding and position?}{1}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid7}{20878196}{8056285}
\pgfsyspdfmark {pgfid10}{34243870}{8068573}
\pgfsyspdfmark {pgfid11}{36357406}{7844659}
\citation{he2015deepresiduallearningimage}
\citation{ba2016layernormalization}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The original transformer architecture, adapted from Vaswani et al.~\cite  {vaswani2017attention}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:transformer}{{1}{2}{The original transformer architecture, adapted from Vaswani et al.~\cite {vaswani2017attention}}{figure.1}{}}
\newlabel{fig:transformer@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{tdo}{\contentsline {todo}{add formulars if space permits}{2}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid12}{20088094}{20088449}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Encoder Stack}{2}{subsection.2.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{why?}{3}{section*.5}\protected@file@percent }
\pgfsyspdfmark {pgfid13}{31409438}{39513565}
\pgfsyspdfmark {pgfid16}{34243870}{39525853}
\pgfsyspdfmark {pgfid17}{36357406}{39301939}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Decoder Stack}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Attention}{3}{subsection.2.4}\protected@file@percent }
\newlabel{eq:attention}{{1}{3}{Attention}{equation.2.1}{}}
\newlabel{eq:attention@cref}{{[equation][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Position-Wise Feed-Forward Networks}{4}{subsection.2.5}\protected@file@percent }
\newlabel{sec:ffn}{{2.5}{4}{Position-Wise Feed-Forward Networks}{subsection.2.5}{}}
\newlabel{sec:ffn@cref}{{[subsection][5][2]2.5}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Normalization Layer}{4}{subsection.2.6}\protected@file@percent }
\newlabel{sec:normalization}{{2.6}{4}{Normalization Layer}{subsection.2.6}{}}
\newlabel{sec:normalization@cref}{{[subsection][6][2]2.6}{[1][4][]4}}
\citation{britz2017massiveexplorationneuralmachine}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Training and Schedule}{5}{subsection.3.2}\protected@file@percent }
\citation{loshchilov2019decoupledweightdecayregularization}
\citation{kingma2017adammethodstochasticoptimization}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}AdamW Optimizer}{6}{subsection.3.3}\protected@file@percent }
\newlabel{eq:mean_var}{{5}{6}{AdamW Optimizer}{equation.3.5}{}}
\newlabel{eq:mean_var@cref}{{[equation][5][]5}{[1][6][]6}}
\newlabel{eq:moments}{{6}{6}{AdamW Optimizer}{equation.3.6}{}}
\newlabel{eq:moments@cref}{{[equation][6][]6}{[1][6][]6}}
\newlabel{eq:weight_decay}{{7}{6}{AdamW Optimizer}{equation.3.7}{}}
\newlabel{eq:weight_decay@cref}{{[equation][7][]7}{[1][6][]6}}
\newlabel{eq:update}{{8}{6}{AdamW Optimizer}{equation.3.8}{}}
\newlabel{eq:update@cref}{{[equation][8][]8}{[1][6][]6}}
\@writefile{tdo}{\contentsline {todo}{Add figure from AdamW paper?}{6}{section*.6}\protected@file@percent }
\pgfsyspdfmark {pgfid19}{20088094}{43356069}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{7}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{7}{Results}{section.4}{}}
\newlabel{sec:results@cref}{{[section][4][]4}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}GPU versus CPU training}{7}{subsection.4.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{why does that happen?}{7}{section*.7}\protected@file@percent }
\pgfsyspdfmark {pgfid20}{10772883}{25466226}
\pgfsyspdfmark {pgfid23}{34243870}{25478514}
\pgfsyspdfmark {pgfid24}{36357406}{25254600}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison of CPU vs. GPU}}{7}{table.1}\protected@file@percent }
\newlabel{tab:comparison}{{1}{7}{Performance comparison of CPU vs. GPU}{table.1}{}}
\newlabel{tab:comparison@cref}{{[table][1][]1}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Machine Translation}{7}{subsection.4.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Where do we apply dropout?}{8}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid25}{20088094}{43356069}
\@writefile{tdo}{\contentsline {todo}{What are learnable parameters in a transformer model?}{8}{section*.9}\protected@file@percent }
\pgfsyspdfmark {pgfid26}{20088094}{42223753}
\@writefile{tdo}{\contentsline {todo}{Inclued questions from tests}{8}{section*.10}\protected@file@percent }
\pgfsyspdfmark {pgfid27}{20088094}{41091437}
\bibdata{references}
\bibcite{britz2017massiveexplorationneuralmachine}{BGLL17}
\bibcite{ba2016layernormalization}{BKH16}
\bibcite{he2015deepresiduallearningimage}{HZRS15}
\bibcite{kingma2017adammethodstochasticoptimization}{KB17}
\bibcite{loshchilov2019decoupledweightdecayregularization}{LH19}
\bibcite{press2017usingoutputembeddingimprove}{PW17}
\bibcite{vaswani2017attention}{Vas17}
\bibstyle{alpha}
\gdef \@abspage@last{10}
