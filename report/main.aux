\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{press2017usingoutputembeddingimprove}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Preprocessing}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Embedding Layers}{1}{subsection.2.2}\protected@file@percent }
\citation{he2015deepresiduallearningimage}
\citation{ba2016layernormalization}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The original transformer architecture, adapted from Vaswani et al.~\cite  {vaswani2017attention}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:transformer}{{1}{2}{The original transformer architecture, adapted from Vaswani et al.~\cite {vaswani2017attention}}{figure.1}{}}
\newlabel{fig:transformer@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Encoder Stack}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decoder Stack}{3}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Attention}{3}{subsection.2.5}\protected@file@percent }
\newlabel{sec:attention}{{2.5}{3}{Attention}{subsection.2.5}{}}
\newlabel{sec:attention@cref}{{[subsection][5][2]2.5}{[1][3][]3}}
\newlabel{eq:attention}{{1}{3}{Attention}{equation.2.1}{}}
\newlabel{eq:attention@cref}{{[equation][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Position-Wise Feed-Forward Networks}{4}{subsection.2.6}\protected@file@percent }
\newlabel{sec:ffn}{{2.6}{4}{Position-Wise Feed-Forward Networks}{subsection.2.6}{}}
\newlabel{sec:ffn@cref}{{[subsection][6][2]2.6}{[1][4][]4}}
\newlabel{eq:ffn}{{2}{4}{Position-Wise Feed-Forward Networks}{equation.2.2}{}}
\newlabel{eq:ffn@cref}{{[equation][2][]2}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Normalization Layer}{4}{subsection.2.7}\protected@file@percent }
\newlabel{sec:normalization}{{2.7}{4}{Normalization Layer}{subsection.2.7}{}}
\newlabel{sec:normalization@cref}{{[subsection][7][2]2.7}{[1][4][]4}}
\newlabel{eq:normalization}{{3}{4}{Normalization Layer}{equation.2.3}{}}
\newlabel{eq:normalization@cref}{{[equation][3][]3}{[1][4][]4}}
\citation{britz2017massiveexplorationneuralmachine}
\citation{loshchilov2019decoupledweightdecayregularization}
\citation{kingma2017adammethodstochasticoptimization}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Training and Schedule}{5}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}AdamW Optimizer}{5}{subsection.3.3}\protected@file@percent }
\newlabel{eq:mean_var}{{5}{5}{AdamW Optimizer}{equation.3.5}{}}
\newlabel{eq:mean_var@cref}{{[equation][5][]5}{[1][5][]5}}
\newlabel{eq:moments}{{6}{6}{AdamW Optimizer}{equation.3.6}{}}
\newlabel{eq:moments@cref}{{[equation][6][]6}{[1][6][]6}}
\newlabel{eq:weight_decay}{{7}{6}{AdamW Optimizer}{equation.3.7}{}}
\newlabel{eq:weight_decay@cref}{{[equation][7][]7}{[1][6][]6}}
\newlabel{eq:update}{{8}{6}{AdamW Optimizer}{equation.3.8}{}}
\newlabel{eq:update@cref}{{[equation][8][]8}{[1][6][]6}}
\@writefile{tdo}{\contentsline {todo}{Add figure from AdamW paper?}{6}{section*.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{20088094}{22879462}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{6}{Results}{section.4}{}}
\newlabel{sec:results@cref}{{[section][4][]4}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}GPU versus CPU Training}{6}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison of CPU vs. GPU}}{7}{table.1}\protected@file@percent }
\newlabel{tab:comparison}{{1}{7}{Performance comparison of CPU vs. GPU}{table.1}{}}
\newlabel{tab:comparison@cref}{{[table][1][]1}{[1][6][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimation of Memory Requirements}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Machine Translation}{7}{subsection.4.3}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Where do we apply dropout?}{8}{section*.2}\protected@file@percent }
\pgfsyspdfmark {pgfid2}{20088094}{43356069}
\@writefile{tdo}{\contentsline {todo}{What are learnable parameters in a transformer model?}{8}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid3}{20088094}{42223753}
\@writefile{tdo}{\contentsline {todo}{Inclued questions from tests}{8}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid4}{20088094}{41091437}
\bibdata{references}
\bibcite{britz2017massiveexplorationneuralmachine}{BGLL17}
\bibcite{ba2016layernormalization}{BKH16}
\bibcite{he2015deepresiduallearningimage}{HZRS15}
\bibcite{kingma2017adammethodstochasticoptimization}{KB17}
\bibcite{loshchilov2019decoupledweightdecayregularization}{LH19}
\bibcite{press2017usingoutputembeddingimprove}{PW17}
\bibcite{vaswani2017attention}{Vas17}
\bibstyle{alpha}
\gdef \@abspage@last{10}
