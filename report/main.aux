\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{press2017usingoutputembeddingimprove}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{1}{section.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{add info about alignment of sequences (maybe add to training section)}{1}{section*.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{20088094}{25607419}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Embedding Layers}{1}{subsection.2.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{delete last point?}{1}{section*.2}\protected@file@percent }
\pgfsyspdfmark {pgfid2}{15069137}{11202013}
\pgfsyspdfmark {pgfid5}{34243870}{11214301}
\pgfsyspdfmark {pgfid6}{36357406}{10990387}
\@writefile{tdo}{\contentsline {todo}{How does the model differentiate between embedding and position?}{1}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid7}{20878196}{8056285}
\pgfsyspdfmark {pgfid10}{34243870}{8068573}
\pgfsyspdfmark {pgfid11}{36357406}{7844659}
\citation{he2015deepresiduallearningimage}
\citation{ba2016layernormalization}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The original transformer architecture, adapted from Vaswani et al.~\cite  {vaswani2017attention}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:transformer}{{1}{2}{The original transformer architecture, adapted from Vaswani et al.~\cite {vaswani2017attention}}{figure.1}{}}
\newlabel{fig:transformer@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{tdo}{\contentsline {todo}{add formulars if space permits}{2}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid12}{20088094}{20088449}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Encoder Stack}{2}{subsection.2.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{why?}{3}{section*.5}\protected@file@percent }
\pgfsyspdfmark {pgfid13}{31409438}{39513565}
\pgfsyspdfmark {pgfid16}{34243870}{39525853}
\pgfsyspdfmark {pgfid17}{36357406}{39301939}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Decoder Stack}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Attention}{3}{subsection.2.4}\protected@file@percent }
\newlabel{eq:attention}{{1}{3}{Attention}{equation.2.1}{}}
\newlabel{eq:attention@cref}{{[equation][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Position-Wise Feed-Forward Networks}{4}{subsection.2.5}\protected@file@percent }
\newlabel{sec:ffn}{{2.5}{4}{Position-Wise Feed-Forward Networks}{subsection.2.5}{}}
\newlabel{sec:ffn@cref}{{[subsection][5][2]2.5}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Normalization Layer}{4}{subsection.2.6}\protected@file@percent }
\newlabel{sec:normalization}{{2.6}{4}{Normalization Layer}{subsection.2.6}{}}
\newlabel{sec:normalization@cref}{{[subsection][6][2]2.6}{[1][4][]4}}
\citation{kingma2017adammethodstochasticoptimization}
\citation{loshchilov2019decoupledweightdecayregularization}
\@writefile{toc}{\contentsline {section}{\numberline {3}Optimization Techniques}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Learning Rate Scheduler}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}AdamW Optimizer}{5}{subsection.3.2}\protected@file@percent }
\newlabel{eq:mean_var}{{5}{5}{AdamW Optimizer}{equation.3.5}{}}
\newlabel{eq:mean_var@cref}{{[equation][5][]5}{[1][5][]5}}
\newlabel{eq:moments}{{6}{5}{AdamW Optimizer}{equation.3.6}{}}
\newlabel{eq:moments@cref}{{[equation][6][]6}{[1][5][]5}}
\newlabel{eq:weight_decay}{{7}{6}{AdamW Optimizer}{equation.3.7}{}}
\newlabel{eq:weight_decay@cref}{{[equation][7][]7}{[1][6][]6}}
\newlabel{eq:update}{{8}{6}{AdamW Optimizer}{equation.3.8}{}}
\newlabel{eq:update@cref}{{[equation][8][]8}{[1][6][]6}}
\@writefile{tdo}{\contentsline {todo}{Add figure from AdamW paper?}{6}{section*.6}\protected@file@percent }
\pgfsyspdfmark {pgfid19}{20088094}{34151895}
\@writefile{toc}{\contentsline {section}{\numberline {4}Training}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Questions}{6}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Practical 4}{6}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Practical 5}{7}{subsection.6.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Where do we apply dropout?}{8}{section*.7}\protected@file@percent }
\pgfsyspdfmark {pgfid20}{20088094}{43356069}
\@writefile{tdo}{\contentsline {todo}{What are learnable parameters in a transformer model?}{8}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid21}{20088094}{42223753}
\@writefile{tdo}{\contentsline {todo}{Inclued questions from tests}{8}{section*.9}\protected@file@percent }
\pgfsyspdfmark {pgfid22}{20088094}{41091437}
\bibdata{references}
\bibcite{ba2016layernormalization}{BKH16}
\bibcite{he2015deepresiduallearningimage}{HZRS15}
\bibcite{kingma2017adammethodstochasticoptimization}{KB17}
\bibcite{loshchilov2019decoupledweightdecayregularization}{LH19}
\bibcite{press2017usingoutputembeddingimprove}{PW17}
\bibcite{vaswani2017attention}{Vas17}
\bibstyle{alpha}
\gdef \@abspage@last{10}
